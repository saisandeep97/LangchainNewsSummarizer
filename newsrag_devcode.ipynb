{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import streamlit as st\n",
    "from datetime import datetime, timedelta\n",
    "from typing import List, Dict\n",
    "import requests\n",
    "from langchain_core.documents import Document\n",
    "from langchain_cohere import CohereEmbeddings\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain_groq import ChatGroq\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "import dotenv\n",
    "import re\n",
    "\n",
    "# Load environment variables\n",
    "dotenv.load_dotenv()\n",
    "\n",
    "# Set environment variables\n",
    "os.environ[\"GROQ_API_KEY\"] = os.getenv(\"GROQ_API_KEY\")\n",
    "os.environ[\"COHERE_API_KEY\"] = os.getenv(\"COHERE_API_KEY\")\n",
    "os.environ[\"NEWSAPI_KEY\"] = os.getenv(\"NEWS_API_KEY\")\n",
    "os.environ['LANGCHAIN_TRACING_V2'] = 'true'\n",
    "os.environ['LANGCHAIN_ENDPOINT'] = 'https://api.smith.langchain.com'\n",
    "os.environ['LANGCHAIN_API_KEY'] = os.getenv(\"LANGCHAIN_API_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NewsCollector:\n",
    "    def __init__(self):\n",
    "        self.base_url = \"https://newsapi.org/v2/top-headlines\"\n",
    "        self.api_key = os.getenv(\"NEWSAPI_KEY\")\n",
    "        \n",
    "    def get_news(self, country: str, category: str) -> List[Dict]:\n",
    "        params = {\n",
    "            \"country\": country,\n",
    "            \"category\": category,\n",
    "            \"apiKey\": self.api_key,\n",
    "            \"pageSize\": 10\n",
    "        }\n",
    "        response = requests.get(self.base_url, params=params)\n",
    "        #print(response.json())\n",
    "        return response.json()[\"articles\"]\n",
    "    \n",
    "    def preprocess_news(self, articles: List[Dict], region: str, category: str) -> List[Document]:\n",
    "        documents = []\n",
    "        for article in articles:\n",
    "            if article['title'] != \"[Removed]\" and article['description'] != \"[Removed]\" and article['content'] != \"[Removed]\":\n",
    "                content = f\"Title: {article['title']}\\nDescription: {article['description']}\\nContent: {article['content']}\"\n",
    "                summary_content = f\"Title: {article['title']}\\nDescription: {article['description']}\"\n",
    "                metadata = {\n",
    "                    \"published_date\": article[\"publishedAt\"],\n",
    "                    \"category\": category,\n",
    "                    \"region\": region,\n",
    "                    \"source\": article[\"source\"][\"name\"],\n",
    "                    \"summary_content\": summary_content  # Store concise version in metadata\n",
    "\n",
    "                }\n",
    "                documents.append(Document(page_content=content, metadata=metadata))\n",
    "        return documents\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "region =\"us\"\n",
    "category = \"business\"\n",
    "news_collector = NewsCollector()\n",
    "articles = news_collector.get_news(region, category)\n",
    "documents = news_collector.preprocess_news(articles,region,category)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VectorStore:\n",
    "    def __init__(self):\n",
    "        self.embeddings = CohereEmbeddings(\n",
    "            model='embed-english-v3.0'\n",
    "        )\n",
    "        self.vectorstore = None\n",
    "    \n",
    "    def create_vectorstore(self, documents: List[Document]):\n",
    "        self.vectorstore = Chroma.from_documents(\n",
    "            documents=documents,\n",
    "            embedding=self.embeddings\n",
    "        )\n",
    "        return self.vectorstore\n",
    "\n",
    "class NewsSummarizer:\n",
    "    def __init__(self):\n",
    "        self.llm = ChatGroq(model=\"llama3-8b-8192\")\n",
    "        \n",
    "    def summarize_news(self, documents: List[Document], current_date: str) -> str:\n",
    "        template = \"\"\"Current date: {current_date}\n",
    "        Based on the following news articles and their published dates, provide a comprehensive summary of the latest news:\n",
    "        \n",
    "        {context}\n",
    "        \n",
    "        Prioritize more recent news while maintaining coherence in the summary.\n",
    "        \"\"\"\n",
    "        \n",
    "        prompt = ChatPromptTemplate.from_template(template)\n",
    "        \n",
    "        chain = (\n",
    "            prompt \n",
    "            | self.llm \n",
    "            | StrOutputParser()\n",
    "        )\n",
    "        \n",
    "        context = \"\\n\\n\".join([\n",
    "            f\"Article ({doc.metadata['published_date']}):\\n{doc.metadata['summary_content']}\"\n",
    "            for doc in documents\n",
    "        ])\n",
    "        \n",
    "        return chain.invoke({\n",
    "            \"context\": context,\n",
    "            \"current_date\": current_date\n",
    "        })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here is a comprehensive summary of the latest news, prioritizing more recent news and maintaining coherence:\n",
      "\n",
      "The past week has been marked by significant developments in the world of finance, technology, and politics. Starting with the cryptocurrency market, BlackRock's accumulating Bitcoin holdings have signaled a bullish trend for the digital currency, sparking hopes of a price surge.\n",
      "\n",
      "In the world of politics, the upcoming US presidential election has taken center stage, with Robinhood offering users the chance to trade contracts for Kamala Harris and Donald Trump starting Monday. Meanwhile, inflation, Big Tech earnings, and a crucial jobs report are set to shape the health of the US economy and stock market, which is trading near record highs.\n",
      "\n",
      "In the tech space, Tesla's stock has boomed after a strong Q3, with Elon Musk predicting 20-30% sales growth next year. The company's stock surge has also led to comparisons with Warren Buffett, with Musk channeling the legendary investor.\n",
      "\n",
      "On the economic front, Volkswagen AG has announced plans to close at least three factories in Germany as part of a cost-cutting push to become more competitive. Boeing workers, on the other hand, are demanding the reinstatement of traditional pension plans, which experts say is a long shot due to the company's shaky financial situation.\n",
      "\n",
      "In other news, more Americans are living paycheck to paycheck than five years ago, according to a new study by the Bank of America Institute. The share of households living paycheck to paycheck has increased across all income brackets since 2019.\n",
      "\n",
      "Finally, MicroStrategy's premium relative to its Bitcoin stack is unlikely to last, according to a report by Steno Research, as the launch of options on spot bitcoin exchange-traded funds in the US will reduce the incentives for investors to hold MicroStrategy stock over these ETFs.\n",
      "\n",
      "The global stock market has responded positively to these developments, with stocks rising ahead of an action-packed week featuring earnings from Wall Street's \"Magnificent 7\" and a crucial jobs report.\n"
     ]
    }
   ],
   "source": [
    "# Create vector store\n",
    "\n",
    "vector_store = VectorStore()\n",
    "vectorstore = vector_store.create_vectorstore(documents)\n",
    "\n",
    "news_summarizer = NewsSummarizer()\n",
    "# Generate summary\n",
    "current_date = datetime.now().strftime(\"%Y-%m-%d\")\n",
    "summary = news_summarizer.summarize_news(documents, current_date)\n",
    "print(summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomNewsRetriever:\n",
    "    def __init__(self, vectorstore):\n",
    "        self.vectorstore = vectorstore\n",
    "        self.llm = ChatGroq(model=\"llama3-8b-8192\")\n",
    "    \n",
    "    def generate_questions(self, user_query: str) -> List[str]:\n",
    "        template = \"\"\"You are an AI language model assistant. Your task is to generate 5 different sub questions OR alternate versions of the given user question to retrieve relevant documents from a vector database.\n",
    "\n",
    "        By generating multiple versions of the user question,\n",
    "        your goal is to help the user overcome some of the limitations\n",
    "        of distance-based similarity search.\n",
    "\n",
    "        By generating sub questions, you can break down questions that refer to multiple concepts into distinct questions. This will help you get the relevant documents for constructing a final answer\n",
    "\n",
    "        If multiple concepts are present in the question, you should break into sub questions, with one question for each concept\n",
    "\n",
    "        Provide these alternative questions separated by newlines between XML tags. For example:\n",
    "\n",
    "        <questions>\n",
    "        - Question 1\n",
    "        - Question 2\n",
    "        - Question 3\n",
    "        </questions>\n",
    "\n",
    "        Original question: {question}\"\"\"\n",
    "        \n",
    "        prompt = ChatPromptTemplate.from_template(template)\n",
    "        chain = prompt | self.llm | StrOutputParser()\n",
    "        \n",
    "        result = chain.invoke({\"question\": user_query})\n",
    "        result = re.search(r'<questions>(.*?)</questions>', result, re.DOTALL).group(1)\n",
    "        return result\n",
    "    \n",
    "    def deduplicate_docs(self, docs: List[Document]) -> List[Document]:\n",
    "        \"\"\"\n",
    "        Deduplicate documents based on their content and metadata\n",
    "        \"\"\"\n",
    "        unique_docs = []\n",
    "        seen_contents = set()\n",
    "        \n",
    "        for doc in docs:\n",
    "            # Create a unique identifier using content and published date\n",
    "            content_identifier = (\n",
    "                doc.page_content,\n",
    "                doc.metadata.get('published_date', '')\n",
    "            )\n",
    "            \n",
    "            if content_identifier not in seen_contents:\n",
    "                seen_contents.add(content_identifier)\n",
    "                unique_docs.append(doc)\n",
    "        \n",
    "        return unique_docs\n",
    "    \n",
    "    def get_relevant_docs(self, questions: List[str]) -> List[Document]:\n",
    "        all_docs = []\n",
    "        for question in questions:\n",
    "            docs = self.vectorstore.similarity_search(question)\n",
    "            all_docs.extend(docs)\n",
    "        \n",
    "        # Deduplicate documents\n",
    "        return self.deduplicate_docs(all_docs)\n",
    "    \n",
    "    def answer_query(self, user_query: str, docs: List[Document], current_date: str) -> str:\n",
    "        template = \"\"\"Current date: {current_date}\n",
    "        Based on the following news articles and their published dates, answer this question: {question}\n",
    "        \n",
    "        Context:\n",
    "        {context}\n",
    "        \n",
    "        Provide a clear and focused answer based on the most recent and relevant information.\n",
    "        \"\"\"\n",
    "        \n",
    "        prompt = ChatPromptTemplate.from_template(template)\n",
    "        chain = prompt | self.llm | StrOutputParser()\n",
    "        \n",
    "        # Use full content for detailed answers to specific queries\n",
    "        context = \"\\n\\n\".join([\n",
    "            f\"Article ({doc.metadata['published_date']}):\\n{doc.page_content}\"\n",
    "            for doc in docs\n",
    "        ])\n",
    "        \n",
    "        return chain.invoke({\n",
    "            \"context\": context,\n",
    "            \"question\": user_query,\n",
    "            \"current_date\": current_date\n",
    "        })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "- What is the latest news on US business trends?\n",
      "- What are the recent developments in the US economy?\n",
      "- What is the current state of the US stock market?\n",
      "- What are the latest updates on US business leaders and executives?\n",
      "- What are the most significant business stories in the US this week?\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Retrying langchain_cohere.embeddings.CohereEmbeddings.embed_with_retry.<locals>._embed_with_retry in 4.0 seconds as it raised TooManyRequestsError: status_code: 429, body: data=None message=\"You are using a Trial key, which is limited to 100 API calls / minute. You can continue to use the Trial key for free or upgrade to a Production key with higher rate limits at 'https://dashboard.cohere.com/api-keys'. Contact us on 'https://discord.gg/XW44jPfYJu' or email us at support@cohere.com with any questions\".\n",
      "Retrying langchain_cohere.embeddings.CohereEmbeddings.embed_with_retry.<locals>._embed_with_retry in 4.0 seconds as it raised TooManyRequestsError: status_code: 429, body: data=None message=\"You are using a Trial key, which is limited to 100 API calls / minute. You can continue to use the Trial key for free or upgrade to a Production key with higher rate limits at 'https://dashboard.cohere.com/api-keys'. Contact us on 'https://discord.gg/XW44jPfYJu' or email us at support@cohere.com with any questions\".\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[24], line 10\u001b[0m\n\u001b[1;32m      7\u001b[0m questions \u001b[38;5;241m=\u001b[39m custom_retriever\u001b[38;5;241m.\u001b[39mgenerate_questions(user_query)\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28mprint\u001b[39m(questions)\n\u001b[0;32m---> 10\u001b[0m relevant_docs \u001b[38;5;241m=\u001b[39m \u001b[43mcustom_retriever\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_relevant_docs\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquestions\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28mprint\u001b[39m(relevant_docs)\n\u001b[1;32m     13\u001b[0m answer \u001b[38;5;241m=\u001b[39m custom_retriever\u001b[38;5;241m.\u001b[39manswer_query(user_query, relevant_docs, current_date)\n",
      "Cell \u001b[0;32mIn[23], line 57\u001b[0m, in \u001b[0;36mCustomNewsRetriever.get_relevant_docs\u001b[0;34m(self, questions)\u001b[0m\n\u001b[1;32m     55\u001b[0m all_docs \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m     56\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m question \u001b[38;5;129;01min\u001b[39;00m questions:\n\u001b[0;32m---> 57\u001b[0m     docs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvectorstore\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msimilarity_search\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquestion\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     58\u001b[0m     all_docs\u001b[38;5;241m.\u001b[39mextend(docs)\n\u001b[1;32m     60\u001b[0m \u001b[38;5;66;03m# Deduplicate documents\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/lcenv/lib/python3.10/site-packages/langchain_community/vectorstores/chroma.py:350\u001b[0m, in \u001b[0;36mChroma.similarity_search\u001b[0;34m(self, query, k, filter, **kwargs)\u001b[0m\n\u001b[1;32m    333\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21msimilarity_search\u001b[39m(\n\u001b[1;32m    334\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    335\u001b[0m     query: \u001b[38;5;28mstr\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    338\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[1;32m    339\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m List[Document]:\n\u001b[1;32m    340\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Run similarity search with Chroma.\u001b[39;00m\n\u001b[1;32m    341\u001b[0m \n\u001b[1;32m    342\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    348\u001b[0m \u001b[38;5;124;03m        List[Document]: List of documents most similar to the query text.\u001b[39;00m\n\u001b[1;32m    349\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 350\u001b[0m     docs_and_scores \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msimilarity_search_with_score\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    351\u001b[0m \u001b[43m        \u001b[49m\u001b[43mquery\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mk\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mfilter\u001b[39;49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mfilter\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    352\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    353\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [doc \u001b[38;5;28;01mfor\u001b[39;00m doc, _ \u001b[38;5;129;01min\u001b[39;00m docs_and_scores]\n",
      "File \u001b[0;32m~/miniconda3/envs/lcenv/lib/python3.10/site-packages/langchain_community/vectorstores/chroma.py:439\u001b[0m, in \u001b[0;36mChroma.similarity_search_with_score\u001b[0;34m(self, query, k, filter, where_document, **kwargs)\u001b[0m\n\u001b[1;32m    431\u001b[0m     results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__query_collection(\n\u001b[1;32m    432\u001b[0m         query_texts\u001b[38;5;241m=\u001b[39m[query],\n\u001b[1;32m    433\u001b[0m         n_results\u001b[38;5;241m=\u001b[39mk,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    436\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m    437\u001b[0m     )\n\u001b[1;32m    438\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 439\u001b[0m     query_embedding \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_embedding_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43membed_query\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquery\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    440\u001b[0m     results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__query_collection(\n\u001b[1;32m    441\u001b[0m         query_embeddings\u001b[38;5;241m=\u001b[39m[query_embedding],\n\u001b[1;32m    442\u001b[0m         n_results\u001b[38;5;241m=\u001b[39mk,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    445\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m    446\u001b[0m     )\n\u001b[1;32m    448\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _results_to_docs_and_scores(results)\n",
      "File \u001b[0;32m~/miniconda3/envs/lcenv/lib/python3.10/site-packages/langchain_cohere/embeddings.py:213\u001b[0m, in \u001b[0;36mCohereEmbeddings.embed_query\u001b[0;34m(self, text)\u001b[0m\n\u001b[1;32m    204\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21membed_query\u001b[39m(\u001b[38;5;28mself\u001b[39m, text: \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m List[\u001b[38;5;28mfloat\u001b[39m]:\n\u001b[1;32m    205\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Call out to Cohere's embedding endpoint.\u001b[39;00m\n\u001b[1;32m    206\u001b[0m \n\u001b[1;32m    207\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    211\u001b[0m \u001b[38;5;124;03m        Embeddings for the text.\u001b[39;00m\n\u001b[1;32m    212\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 213\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43membed\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minput_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43msearch_query\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m[\u001b[38;5;241m0\u001b[39m]\n",
      "File \u001b[0;32m~/miniconda3/envs/lcenv/lib/python3.10/site-packages/langchain_cohere/embeddings.py:142\u001b[0m, in \u001b[0;36mCohereEmbeddings.embed\u001b[0;34m(self, texts, input_type)\u001b[0m\n\u001b[1;32m    136\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21membed\u001b[39m(\n\u001b[1;32m    137\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    138\u001b[0m     texts: List[\u001b[38;5;28mstr\u001b[39m],\n\u001b[1;32m    139\u001b[0m     \u001b[38;5;241m*\u001b[39m,\n\u001b[1;32m    140\u001b[0m     input_type: typing\u001b[38;5;241m.\u001b[39mOptional[cohere\u001b[38;5;241m.\u001b[39mEmbedInputType] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    141\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m List[List[\u001b[38;5;28mfloat\u001b[39m]]:\n\u001b[0;32m--> 142\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43membed_with_retry\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    143\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    144\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtexts\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtexts\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    145\u001b[0m \u001b[43m        \u001b[49m\u001b[43minput_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    146\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtruncate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtruncate\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    147\u001b[0m \u001b[43m        \u001b[49m\u001b[43membedding_types\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43membedding_types\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    148\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    149\u001b[0m     embeddings \u001b[38;5;241m=\u001b[39m response\u001b[38;5;241m.\u001b[39mdict()\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124membeddings\u001b[39m\u001b[38;5;124m\"\u001b[39m, [])\n\u001b[1;32m    150\u001b[0m     embeddings_as_float: List[List[\u001b[38;5;28mfloat\u001b[39m]] \u001b[38;5;241m=\u001b[39m []\n",
      "File \u001b[0;32m~/miniconda3/envs/lcenv/lib/python3.10/site-packages/langchain_cohere/embeddings.py:124\u001b[0m, in \u001b[0;36mCohereEmbeddings.embed_with_retry\u001b[0;34m(self, **kwargs)\u001b[0m\n\u001b[1;32m    120\u001b[0m \u001b[38;5;129m@retry_decorator\u001b[39m\n\u001b[1;32m    121\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_embed_with_retry\u001b[39m(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[1;32m    122\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclient\u001b[38;5;241m.\u001b[39membed(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m--> 124\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_embed_with_retry\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/lcenv/lib/python3.10/site-packages/tenacity/__init__.py:336\u001b[0m, in \u001b[0;36mBaseRetrying.wraps.<locals>.wrapped_f\u001b[0;34m(*args, **kw)\u001b[0m\n\u001b[1;32m    334\u001b[0m copy \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcopy()\n\u001b[1;32m    335\u001b[0m wrapped_f\u001b[38;5;241m.\u001b[39mstatistics \u001b[38;5;241m=\u001b[39m copy\u001b[38;5;241m.\u001b[39mstatistics  \u001b[38;5;66;03m# type: ignore[attr-defined]\u001b[39;00m\n\u001b[0;32m--> 336\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcopy\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkw\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/lcenv/lib/python3.10/site-packages/tenacity/__init__.py:475\u001b[0m, in \u001b[0;36mRetrying.__call__\u001b[0;34m(self, fn, *args, **kwargs)\u001b[0m\n\u001b[1;32m    473\u001b[0m retry_state \u001b[38;5;241m=\u001b[39m RetryCallState(retry_object\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m, fn\u001b[38;5;241m=\u001b[39mfn, args\u001b[38;5;241m=\u001b[39margs, kwargs\u001b[38;5;241m=\u001b[39mkwargs)\n\u001b[1;32m    474\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m--> 475\u001b[0m     do \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43miter\u001b[49m\u001b[43m(\u001b[49m\u001b[43mretry_state\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mretry_state\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    476\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(do, DoAttempt):\n\u001b[1;32m    477\u001b[0m         \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[0;32m~/miniconda3/envs/lcenv/lib/python3.10/site-packages/tenacity/__init__.py:376\u001b[0m, in \u001b[0;36mBaseRetrying.iter\u001b[0;34m(self, retry_state)\u001b[0m\n\u001b[1;32m    374\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    375\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m action \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39miter_state\u001b[38;5;241m.\u001b[39mactions:\n\u001b[0;32m--> 376\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43maction\u001b[49m\u001b[43m(\u001b[49m\u001b[43mretry_state\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    377\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "File \u001b[0;32m~/miniconda3/envs/lcenv/lib/python3.10/site-packages/tenacity/__init__.py:398\u001b[0m, in \u001b[0;36mBaseRetrying._post_retry_check_actions.<locals>.<lambda>\u001b[0;34m(rs)\u001b[0m\n\u001b[1;32m    396\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_post_retry_check_actions\u001b[39m(\u001b[38;5;28mself\u001b[39m, retry_state: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRetryCallState\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    397\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39miter_state\u001b[38;5;241m.\u001b[39mis_explicit_retry \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39miter_state\u001b[38;5;241m.\u001b[39mretry_run_result):\n\u001b[0;32m--> 398\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_add_action_func(\u001b[38;5;28;01mlambda\u001b[39;00m rs: \u001b[43mrs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moutcome\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mresult\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m    399\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[1;32m    401\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mafter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/miniconda3/envs/lcenv/lib/python3.10/concurrent/futures/_base.py:451\u001b[0m, in \u001b[0;36mFuture.result\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    449\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m CancelledError()\n\u001b[1;32m    450\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_state \u001b[38;5;241m==\u001b[39m FINISHED:\n\u001b[0;32m--> 451\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__get_result\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    453\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_condition\u001b[38;5;241m.\u001b[39mwait(timeout)\n\u001b[1;32m    455\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_state \u001b[38;5;129;01min\u001b[39;00m [CANCELLED, CANCELLED_AND_NOTIFIED]:\n",
      "File \u001b[0;32m~/miniconda3/envs/lcenv/lib/python3.10/concurrent/futures/_base.py:403\u001b[0m, in \u001b[0;36mFuture.__get_result\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    401\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exception:\n\u001b[1;32m    402\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 403\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exception\n\u001b[1;32m    404\u001b[0m     \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    405\u001b[0m         \u001b[38;5;66;03m# Break a reference cycle with the exception in self._exception\u001b[39;00m\n\u001b[1;32m    406\u001b[0m         \u001b[38;5;28mself\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/lcenv/lib/python3.10/site-packages/tenacity/__init__.py:478\u001b[0m, in \u001b[0;36mRetrying.__call__\u001b[0;34m(self, fn, *args, **kwargs)\u001b[0m\n\u001b[1;32m    476\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(do, DoAttempt):\n\u001b[1;32m    477\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 478\u001b[0m         result \u001b[38;5;241m=\u001b[39m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    479\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m:  \u001b[38;5;66;03m# noqa: B902\u001b[39;00m\n\u001b[1;32m    480\u001b[0m         retry_state\u001b[38;5;241m.\u001b[39mset_exception(sys\u001b[38;5;241m.\u001b[39mexc_info())  \u001b[38;5;66;03m# type: ignore[arg-type]\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/lcenv/lib/python3.10/site-packages/langchain_cohere/embeddings.py:122\u001b[0m, in \u001b[0;36mCohereEmbeddings.embed_with_retry.<locals>._embed_with_retry\u001b[0;34m(**kwargs)\u001b[0m\n\u001b[1;32m    120\u001b[0m \u001b[38;5;129m@retry_decorator\u001b[39m\n\u001b[1;32m    121\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_embed_with_retry\u001b[39m(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[0;32m--> 122\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mclient\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43membed\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/lcenv/lib/python3.10/site-packages/cohere/client.py:206\u001b[0m, in \u001b[0;36mClient.embed\u001b[0;34m(self, texts, images, model, input_type, embedding_types, truncate, request_options, batching)\u001b[0m\n\u001b[1;32m    203\u001b[0m textsarr: typing\u001b[38;5;241m.\u001b[39mSequence[\u001b[38;5;28mstr\u001b[39m]  \u001b[38;5;241m=\u001b[39m texts \u001b[38;5;28;01mif\u001b[39;00m texts \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m OMIT \u001b[38;5;129;01mand\u001b[39;00m texts \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m []\n\u001b[1;32m    204\u001b[0m texts_batches \u001b[38;5;241m=\u001b[39m [textsarr[i : i \u001b[38;5;241m+\u001b[39m embed_batch_size] \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;28mlen\u001b[39m(textsarr), embed_batch_size)]\n\u001b[0;32m--> 206\u001b[0m responses \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m    207\u001b[0m     response\n\u001b[1;32m    208\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m response \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_executor\u001b[38;5;241m.\u001b[39mmap(\n\u001b[1;32m    209\u001b[0m         \u001b[38;5;28;01mlambda\u001b[39;00m text_batch: BaseCohere\u001b[38;5;241m.\u001b[39membed(\n\u001b[1;32m    210\u001b[0m             \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    211\u001b[0m             texts\u001b[38;5;241m=\u001b[39mtext_batch,\n\u001b[1;32m    212\u001b[0m             model\u001b[38;5;241m=\u001b[39mmodel,\n\u001b[1;32m    213\u001b[0m             input_type\u001b[38;5;241m=\u001b[39minput_type,\n\u001b[1;32m    214\u001b[0m             embedding_types\u001b[38;5;241m=\u001b[39membedding_types,\n\u001b[1;32m    215\u001b[0m             truncate\u001b[38;5;241m=\u001b[39mtruncate,\n\u001b[1;32m    216\u001b[0m             request_options\u001b[38;5;241m=\u001b[39mrequest_options,\n\u001b[1;32m    217\u001b[0m         ),\n\u001b[1;32m    218\u001b[0m         texts_batches,\n\u001b[1;32m    219\u001b[0m     )\n\u001b[1;32m    220\u001b[0m ]\n\u001b[1;32m    222\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m merge_embed_responses(responses)\n",
      "File \u001b[0;32m~/miniconda3/envs/lcenv/lib/python3.10/site-packages/cohere/client.py:206\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    203\u001b[0m textsarr: typing\u001b[38;5;241m.\u001b[39mSequence[\u001b[38;5;28mstr\u001b[39m]  \u001b[38;5;241m=\u001b[39m texts \u001b[38;5;28;01mif\u001b[39;00m texts \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m OMIT \u001b[38;5;129;01mand\u001b[39;00m texts \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m []\n\u001b[1;32m    204\u001b[0m texts_batches \u001b[38;5;241m=\u001b[39m [textsarr[i : i \u001b[38;5;241m+\u001b[39m embed_batch_size] \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;28mlen\u001b[39m(textsarr), embed_batch_size)]\n\u001b[0;32m--> 206\u001b[0m responses \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m    207\u001b[0m     response\n\u001b[1;32m    208\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m response \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_executor\u001b[38;5;241m.\u001b[39mmap(\n\u001b[1;32m    209\u001b[0m         \u001b[38;5;28;01mlambda\u001b[39;00m text_batch: BaseCohere\u001b[38;5;241m.\u001b[39membed(\n\u001b[1;32m    210\u001b[0m             \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    211\u001b[0m             texts\u001b[38;5;241m=\u001b[39mtext_batch,\n\u001b[1;32m    212\u001b[0m             model\u001b[38;5;241m=\u001b[39mmodel,\n\u001b[1;32m    213\u001b[0m             input_type\u001b[38;5;241m=\u001b[39minput_type,\n\u001b[1;32m    214\u001b[0m             embedding_types\u001b[38;5;241m=\u001b[39membedding_types,\n\u001b[1;32m    215\u001b[0m             truncate\u001b[38;5;241m=\u001b[39mtruncate,\n\u001b[1;32m    216\u001b[0m             request_options\u001b[38;5;241m=\u001b[39mrequest_options,\n\u001b[1;32m    217\u001b[0m         ),\n\u001b[1;32m    218\u001b[0m         texts_batches,\n\u001b[1;32m    219\u001b[0m     )\n\u001b[1;32m    220\u001b[0m ]\n\u001b[1;32m    222\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m merge_embed_responses(responses)\n",
      "File \u001b[0;32m~/miniconda3/envs/lcenv/lib/python3.10/concurrent/futures/_base.py:621\u001b[0m, in \u001b[0;36mExecutor.map.<locals>.result_iterator\u001b[0;34m()\u001b[0m\n\u001b[1;32m    618\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m fs:\n\u001b[1;32m    619\u001b[0m     \u001b[38;5;66;03m# Careful not to keep a reference to the popped future\u001b[39;00m\n\u001b[1;32m    620\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m timeout \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 621\u001b[0m         \u001b[38;5;28;01myield\u001b[39;00m \u001b[43m_result_or_cancel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpop\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    622\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    623\u001b[0m         \u001b[38;5;28;01myield\u001b[39;00m _result_or_cancel(fs\u001b[38;5;241m.\u001b[39mpop(), end_time \u001b[38;5;241m-\u001b[39m time\u001b[38;5;241m.\u001b[39mmonotonic())\n",
      "File \u001b[0;32m~/miniconda3/envs/lcenv/lib/python3.10/concurrent/futures/_base.py:319\u001b[0m, in \u001b[0;36m_result_or_cancel\u001b[0;34m(***failed resolving arguments***)\u001b[0m\n\u001b[1;32m    317\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    318\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 319\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfut\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mresult\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    320\u001b[0m     \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    321\u001b[0m         fut\u001b[38;5;241m.\u001b[39mcancel()\n",
      "File \u001b[0;32m~/miniconda3/envs/lcenv/lib/python3.10/concurrent/futures/_base.py:453\u001b[0m, in \u001b[0;36mFuture.result\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    450\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_state \u001b[38;5;241m==\u001b[39m FINISHED:\n\u001b[1;32m    451\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__get_result()\n\u001b[0;32m--> 453\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_condition\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwait\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    455\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_state \u001b[38;5;129;01min\u001b[39;00m [CANCELLED, CANCELLED_AND_NOTIFIED]:\n\u001b[1;32m    456\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m CancelledError()\n",
      "File \u001b[0;32m~/miniconda3/envs/lcenv/lib/python3.10/threading.py:320\u001b[0m, in \u001b[0;36mCondition.wait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    318\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:    \u001b[38;5;66;03m# restore state no matter what (e.g., KeyboardInterrupt)\u001b[39;00m\n\u001b[1;32m    319\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m timeout \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 320\u001b[0m         \u001b[43mwaiter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43macquire\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    321\u001b[0m         gotit \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m    322\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "current_date = datetime.now().strftime(\"%Y-%m-%d\")\n",
    "\n",
    "user_query = \"What is the latest news on business in the US?\"\n",
    "\n",
    "custom_retriever = CustomNewsRetriever(vector_store.vectorstore)\n",
    "\n",
    "questions = custom_retriever.generate_questions(user_query)\n",
    "print(questions)\n",
    "\n",
    "relevant_docs = custom_retriever.get_relevant_docs(questions)\n",
    "print(relevant_docs)\n",
    "\n",
    "answer = custom_retriever.answer_query(user_query, relevant_docs, current_date)\n",
    "print(answer)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lcenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
